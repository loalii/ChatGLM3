{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from ChatGLM3 import ChatGLM3\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from Tool.Weather import Weather\n",
    "from Tool.Calculator import Calculator\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "import os\n",
    "\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"WXKQ33-TRKT5JXK92\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08f211c4a22483a8dda34dc9be2d031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_tool(tools, llm, prompt_chain: List[str]):\n",
    "    loaded_tolls = []\n",
    "    for tool in tools:\n",
    "        if isinstance(tool, str):\n",
    "            loaded_tolls.append(load_tools([tool], llm=llm)[0])\n",
    "        else:\n",
    "            loaded_tolls.append(tool)\n",
    "    agent = initialize_agent(\n",
    "        loaded_tolls, llm,\n",
    "        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    for prompt in prompt_chain:\n",
    "        agent.run(prompt)\n",
    "\n",
    "model_path = \"/share/lilin/chatglm3-6b\"\n",
    "llm = ChatGLM3()\n",
    "llm.load_model(model_name_or_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tool(tools, llm, prompt_chain: List[str]):\n",
    "    loaded_tolls = []\n",
    "    for tool in tools:\n",
    "        if isinstance(tool, str):\n",
    "            loaded_tolls.append(load_tools([tool], llm=llm)[0])\n",
    "        else:\n",
    "            loaded_tolls.append(tool)\n",
    "    agent = initialize_agent(\n",
    "        loaded_tolls, llm,\n",
    "        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    for prompt in prompt_chain:\n",
    "        agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "======\n",
      "System: Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
      "\n",
      "Calculator: Useful for when you need to answer questions about math, args: {{'para': {{'title': 'Para', 'type': 'string'}}}}\n",
      "\n",
      "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
      "\n",
      "Valid \"action\" values: \"Final Answer\" or Calculator\n",
      "\n",
      "Provide only ONE action per $JSON_BLOB, as shown:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "\n",
      "Follow this format:\n",
      "\n",
      "Question: input question to answer\n",
      "Thought: consider previous and subsequent steps\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: action result\n",
      "... (repeat Thought/Action/Observation N times)\n",
      "Thought: I know what to respond\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Final response to human\"\n",
      "}\n",
      "```\n",
      "\n",
      "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\n",
      "Thought:\n",
      "Human: 根号下(23124*9545)是多少\n",
      "\n",
      "\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Action: \n",
      "```\n",
      "{\"action\": \"Calculator\", \"action_input\": \"23124*9545\"}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m220718580\u001b[0m\n",
      "Thought:======\n",
      "System: Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
      "\n",
      "Calculator: Useful for when you need to answer questions about math, args: {{'para': {{'title': 'Para', 'type': 'string'}}}}\n",
      "\n",
      "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
      "\n",
      "Valid \"action\" values: \"Final Answer\" or Calculator\n",
      "\n",
      "Provide only ONE action per $JSON_BLOB, as shown:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}\n",
      "```\n",
      "\n",
      "Follow this format:\n",
      "\n",
      "Question: input question to answer\n",
      "Thought: consider previous and subsequent steps\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: action result\n",
      "... (repeat Thought/Action/Observation N times)\n",
      "Thought: I know what to respond\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Final response to human\"\n",
      "}\n",
      "```\n",
      "\n",
      "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\n",
      "Thought:\n",
      "Human: 根号下(23124*9545)是多少\n",
      "\n",
      "This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\n",
      "\n",
      "Action: \n",
      "```\n",
      "{\"action\": \"Calculator\", \"action_input\": \"23124*9545\"}\n",
      "```\n",
      "Observation: 220718580\n",
      "Thought:\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Action: \n",
      "```\n",
      "{\"action\": \"Final Answer\", \"action_input\": \"根号下(23124*9545)等于220718580。\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# arxiv: 单个工具调用示例 1\n",
    "# run_tool([\"arxiv\"], llm, [\n",
    "#     \"帮我查询GLM-130B相关工作\"\n",
    "# ])\n",
    "\n",
    "run_tool([Calculator()], llm, [\n",
    "    \"根号下(23124*9545)是多少\"\n",
    "])\n",
    "\n",
    "# weather: 单个工具调用示例 2\n",
    "# run_tool([Weather()], llm, [\n",
    "#     \"今天北京天气怎么样？\",\n",
    "#     \"What's the weather like in Shanghai today\",\n",
    "# ])\n",
    "\n",
    "# calculator: 单个工具调用示例 3\n",
    "# run_tool([Calculator()], llm, [\n",
    "#     \"12345679乘以54等于多少？\",\n",
    "#     \"3.14的3.14次方等于多少？\",\n",
    "#     \"根号2加上根号三等于多少？\",\n",
    "# ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm3_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
