{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "import logging\n",
    "import datetime\n",
    "log_time = datetime.datetime.now()\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "DATE_FORMAT = \"%m/%d/%Y %H:%M:%S %p\"\n",
    "logging.basicConfig(filename=f\"./logger/{log_time}.log\", level=logging.INFO, format=LOG_FORMAT,datefmt=DATE_FORMAT)\n",
    "logging.info(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[registered tool] {'description': '随机生成一个数x, 使得 `range[0]` <= x < `range[1]`， 随机数生成的种子使用 `seed`',\n",
      " 'name': 'random_number_generator',\n",
      " 'params': [{'description': '随机数生成器使用的种子',\n",
      "             'name': 'seed',\n",
      "             'required': True,\n",
      "             'type': 'int'},\n",
      "            {'description': '生成随机数的范围',\n",
      "             'name': 'range',\n",
      "             'required': True,\n",
      "             'type': 'tuple[int, int]'}]}\n",
      "[registered tool] {'description': '获取句子 `input_text` 的长度',\n",
      " 'name': 'get_sentence_length',\n",
      " 'params': [{'description': '输入的句子',\n",
      "             'name': 'input_text',\n",
      "             'required': True,\n",
      "             'type': 'str'}]}\n",
      "[registered tool] {'description': '返回指数计算的结果，底数 `base` 的指数 `power` 次方',\n",
      " 'name': 'exponentiation_calculation',\n",
      " 'params': [{'description': '底数',\n",
      "             'name': 'base',\n",
      "             'required': True,\n",
      "             'type': 'int'},\n",
      "            {'description': '指数',\n",
      "             'name': 'power',\n",
      "             'required': True,\n",
      "             'type': 'int'}]}\n",
      "[registered tool] {'description': '从网络上获得 `keyword` 的习惯内容信息。\\n'\n",
      "                '在你要回答你现有知识无法回答的问题时，你应该使用这个工具（尤其是当你需要获得最新的实时信息，或者你缺少相关信息时，在这种情况下请更倾向于使用这个工具）。',\n",
      " 'name': 'web_search',\n",
      " 'params': [{'description': '搜索使用的关键字',\n",
      "             'name': 'keyword',\n",
      "             'required': True,\n",
      "             'type': 'str'}]}\n",
      "[registered tool] {'description': 'Get the current weather for `city_name`',\n",
      " 'name': 'get_weather',\n",
      " 'params': [{'description': 'The name of the city to be queried',\n",
      "             'name': 'city_name',\n",
      "             'required': True,\n",
      "             'type': 'str'}]}\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "import os\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "from huggingface_hub.inference._text_generation import TextGenerationStreamResponse, Token\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Protocol\n",
    "\n",
    "import json\n",
    "import re\n",
    "from conversation import postprocess_text, preprocess_text, Conversation, Role\n",
    "from tool_registry import dispatch_tool, get_tools\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"]='http://10.10.20.100:1089'\n",
    "os.environ[\"HTTPS_PROXY\"]='http://10.10.20.100:1089'\n",
    "\n",
    "TOOL_PROMPT = 'Answer the following questions as best as you can. You have access to the following tools:'\n",
    "\n",
    "# MODEL_PATH = os.environ.get('MODEL_PATH', '/share/lilin/chatglm3-6b')\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH', '/share/lilin/ChatGLM3/finetune_demo/output/tool_alpaca_ft-20231110-172112-1e-4/checkpoint-200')\n",
    "# MODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/chatglm3-6b')\n",
    "PT_PATH = os.environ.get('PT_PATH', None)       # 不使用checkpoint\n",
    "TOKENIZER_PATH = os.environ.get(\"TOKENIZER_PATH\", '/share/lilin/chatglm3-6b')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/share/lilin/chatglm3-6b\", trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(\"/share/lilin/chatglm3-6b\", trust_remote_code=True, device='cuda')\n",
    "# model = model.eval()\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "# print(response)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the configuration_chatglm.py inside /share/lilin/ChatGLM3/finetune_demo/output/tool_alpaca_ft-20231110-172112-1e-4/checkpoint-200.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/share/lilin/ChatGLM3/finetune_demo/output/tool_alpaca_ft-20231110-172112-1e-4/checkpoint-200 does not appear to have a file named configuration_chatglm.py. Checkout 'https://huggingface.co//share/lilin/ChatGLM3/finetune_demo/output/tool_alpaca_ft-20231110-172112-1e-4/checkpoint-200/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/share/lilin/ChatGLM3/composite_demo/test.ipynb 单元格 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m             text \u001b[39m=\u001b[39m new_text\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m             \u001b[39myield\u001b[39;00m TextGenerationStreamResponse(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m                 generated_text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m                 token\u001b[39m=\u001b[39mToken(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m                 )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m             )\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m client \u001b[39m=\u001b[39m HFClient(MODEL_PATH, TOKENIZER_PATH, PT_PATH)\n",
      "\u001b[1;32m/share/lilin/ChatGLM3/composite_demo/test.ipynb 单元格 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mprefix_encoder\u001b[39m.\u001b[39mload_state_dict(new_prefix_state_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22696e66696e6941495f4133227d/share/lilin/ChatGLM3/composite_demo/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m )\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:525\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    526\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    527\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    528\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    529\u001b[0m     code_revision\u001b[39m=\u001b[39;49mcode_revision,\n\u001b[1;32m    530\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    532\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1043\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m has_remote_code \u001b[39mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m   1042\u001b[0m     class_ref \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 1043\u001b[0m     config_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m   1044\u001b[0m         class_ref, pretrained_model_name_or_path, code_revision\u001b[39m=\u001b[39;49mcode_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1045\u001b[0m     )\n\u001b[1;32m   1046\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[1;32m   1047\u001b[0m         config_class\u001b[39m.\u001b[39mregister_for_auto_class()\n",
      "File \u001b[0;32m/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:485\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     code_revision \u001b[39m=\u001b[39m revision\n\u001b[1;32m    484\u001b[0m \u001b[39m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m final_module \u001b[39m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    486\u001b[0m     repo_id,\n\u001b[1;32m    487\u001b[0m     module_file \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.py\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    488\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    489\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    490\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    491\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    492\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    493\u001b[0m     revision\u001b[39m=\u001b[39;49mcode_revision,\n\u001b[1;32m    494\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    495\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    496\u001b[0m )\n\u001b[1;32m    497\u001b[0m \u001b[39mreturn\u001b[39;00m get_class_in_module(class_name, final_module\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:292\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m new_files \u001b[39m=\u001b[39m []\n\u001b[1;32m    290\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     resolved_module_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    293\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    294\u001b[0m         module_file,\n\u001b[1;32m    295\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    296\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    297\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    298\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    299\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    300\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    301\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    302\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    303\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_local \u001b[39mand\u001b[39;00m cached_module \u001b[39m!=\u001b[39m resolved_module_file:\n\u001b[1;32m    306\u001b[0m         new_files\u001b[39m.\u001b[39mappend(module_file)\n",
      "File \u001b[0;32m/share/lilin/anaconda3/envs/glm3_demo/lib/python3.10/site-packages/transformers/utils/hub.py:400\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    399\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 400\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /share/lilin/ChatGLM3/finetune_demo/output/tool_alpaca_ft-20231110-172112-1e-4/checkpoint-200 does not appear to have a file named configuration_chatglm.py. Checkout 'https://huggingface.co//share/lilin/ChatGLM3/finetune_demo/output/tool_alpaca_ft-20231110-172112-1e-4/checkpoint-200/None' for available files."
     ]
    }
   ],
   "source": [
    "# 挂起llm\n",
    "def stream_chat(model, tokenizer, query: str, history: list[tuple[str, str]] = None, role: str = \"user\",\n",
    "                    past_key_values=None,max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8,\n",
    "                    logits_processor=None, return_past_key_values=False, **kwargs):\n",
    "        \n",
    "    from transformers.generation.logits_process import LogitsProcessor\n",
    "    from transformers.generation.utils import LogitsProcessorList\n",
    "\n",
    "    class InvalidScoreLogitsProcessor(LogitsProcessor):\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "            if torch.isnan(scores).any() or torch.isinf(scores).any():\n",
    "                scores.zero_()\n",
    "                scores[..., 5] = 5e4\n",
    "            return scores\n",
    "\n",
    "    if history is None:\n",
    "        history = []\n",
    "    if logits_processor is None:\n",
    "        logits_processor = LogitsProcessorList()\n",
    "    logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "    eos_token_id = [tokenizer.eos_token_id, tokenizer.get_command(\"<|user|>\"),\n",
    "                    tokenizer.get_command(\"<|observation|>\")]\n",
    "    gen_kwargs = {\"max_length\": max_length, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "                    \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n",
    "    if past_key_values is None:\n",
    "        inputs = tokenizer.build_chat_input(query, history=history, role=role)\n",
    "    else:\n",
    "        inputs = tokenizer.build_chat_input(query, role=role)\n",
    "    inputs = inputs.to(model.device)\n",
    "    if past_key_values is not None:\n",
    "        past_length = past_key_values[0][0].shape[0]\n",
    "        if model.transformer.pre_seq_len is not None:\n",
    "            past_length -= model.transformer.pre_seq_len\n",
    "        inputs.position_ids += past_length\n",
    "        attention_mask = inputs.attention_mask\n",
    "        attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    history.append({\"role\": role, \"content\": query})\n",
    "    for outputs in model.stream_generate(**inputs, past_key_values=past_key_values,\n",
    "                                        eos_token_id=eos_token_id, return_past_key_values=return_past_key_values,\n",
    "                                        **gen_kwargs):\n",
    "        if return_past_key_values:\n",
    "            outputs, past_key_values = outputs\n",
    "        outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):]\n",
    "        response = tokenizer.decode(outputs)\n",
    "        if response and response[-1] != \"�\":\n",
    "            new_history = history\n",
    "            if return_past_key_values:\n",
    "                yield response, new_history, past_key_values\n",
    "            else:\n",
    "                yield response, new_history\n",
    "\n",
    "# class Client(Protocol):         # 协议类只用于代码静态检查\n",
    "#     def generate_stream(self,\n",
    "#         system: str | None,\n",
    "#         tools: list[dict] | None,\n",
    "#         history: list[Conversation],\n",
    "#         **parameters: Any\n",
    "#     ) -> Iterable[TextGenerationStreamResponse]:\n",
    "#         ...\n",
    "\n",
    "OBS_PROMPT = \"You have used tools and got the related information. Using the following tool results answering the previous questions: \"\n",
    "\n",
    "\n",
    "class HFClient:\n",
    "    def __init__(self, model_path: str, tokenizer_path: str, pt_checkpoint: str | None = None,):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "\n",
    "        if pt_checkpoint is not None:\n",
    "            config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)\n",
    "            self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True, config=config)\n",
    "            prefix_state_dict = torch.load(os.path.join(pt_checkpoint, \"pytorch_model.bin\"))\n",
    "            new_prefix_state_dict = {}\n",
    "            for k, v in prefix_state_dict.items():\n",
    "                if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "                    new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "            print(\"Loaded from pt checkpoints\", new_prefix_state_dict.keys())\n",
    "            self.model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        self.model = self.model.to(\n",
    "            'cuda' if torch.cuda.is_available() else\n",
    "            'mps' if torch.backends.mps.is_available() else\n",
    "            'cpu'\n",
    "        ).eval()\n",
    "\n",
    "\n",
    "    def generate_stream(self,\n",
    "        system: str | None,\n",
    "        tools: list[dict] | None,\n",
    "        history: list[Conversation],\n",
    "        **parameters: Any\n",
    "    ) -> Iterable[TextGenerationStreamResponse]:\n",
    "\n",
    "        chat_history = []\n",
    "\n",
    "        for conversation in history[:-1]:\n",
    "            chat_history.append({\n",
    "                'role': str(conversation.role).removeprefix('<|').removesuffix('|>'),\n",
    "                'content': conversation.content,\n",
    "            })\n",
    "        \n",
    "        chat_history.append({\n",
    "            'role': 'system',\n",
    "            'content': OBS_PROMPT if history[-1].role==Role.OBSERVATION else TOOL_PROMPT,\n",
    "        })\n",
    "\n",
    "        if tools:\n",
    "            chat_history[-1]['tools'] = tools\n",
    "\n",
    "        # chat_history.append({\n",
    "        #         'role': str(history[-1].role).removeprefix('<|').removesuffix('|>'),\n",
    "        #         'content': history[-1].content,\n",
    "        #     })\n",
    "\n",
    "\n",
    "        query = history[-1].content\n",
    "        role = str(history[-1].role).removeprefix('<|').removesuffix('|>')\n",
    "\n",
    "        text = ''\n",
    "        \n",
    "        for new_text, _ in stream_chat(self.model,\n",
    "            self.tokenizer,\n",
    "            query,\n",
    "            chat_history,\n",
    "            role,\n",
    "            **parameters,\n",
    "        ):\n",
    "            word = new_text.removeprefix(text)\n",
    "            word_stripped = word.strip()\n",
    "            text = new_text\n",
    "            yield TextGenerationStreamResponse(\n",
    "                generated_text=text,\n",
    "                token=Token(\n",
    "                    id=0,\n",
    "                    logprob=0,\n",
    "                    text=word,\n",
    "                    special=word_stripped.startswith('<|') and word_stripped.endswith('|>'),\n",
    "                )\n",
    "            )\n",
    "\n",
    "client = HFClient(MODEL_PATH, TOKENIZER_PATH, PT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===TOOLS  {'random_number_generator': {'name': 'random_number_generator', 'description': '随机生成一个数x, 使得 `range[0]` <= x < `range[1]`， 随机数生成的种子使用 `seed`', 'params': [{'name': 'seed', 'description': '随机数生成器使用的种子', 'type': 'int', 'required': True}, {'name': 'range', 'description': '生成随机数的范围', 'type': 'tuple[int, int]', 'required': True}]}, 'get_sentence_length': {'name': 'get_sentence_length', 'description': '获取句子 `input_text` 的长度', 'params': [{'name': 'input_text', 'description': '输入的句子', 'type': 'str', 'required': True}]}, 'exponentiation_calculation': {'name': 'exponentiation_calculation', 'description': '返回指数计算的结果，底数 `base` 的指数 `power` 次方', 'params': [{'name': 'base', 'description': '底数', 'type': 'int', 'required': True}, {'name': 'power', 'description': '指数', 'type': 'int', 'required': True}]}, 'web_search': {'name': 'web_search', 'description': '从网络上获得 `keyword` 的习惯内容信息。\\n在你要回答你现有知识无法回答的问题时，你应该使用这个工具tool（尤其是当你需要获得最新的实时信息，或者你缺少相关信息时，在这种情况下请更倾向于使用他）。', 'params': [{'name': 'keyword', 'description': '搜索使用的关键字', 'type': 'str', 'required': True}]}, 'get_weather': {'name': 'get_weather', 'description': 'Get the current weather for `city_name`', 'params': [{'name': 'city_name', 'description': 'The name of the city to be queried', 'type': 'str', 'required': True}]}}\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "TOP_P = 0.8\n",
    "TEMPERATURE = 0.01\n",
    "MAX_LENGTH = 8192\n",
    "TRUNCATE_LENGTH = 1024\n",
    "\n",
    "\n",
    "def append_conversation(\n",
    "    conversation: Conversation,\n",
    "    history: list[Conversation],\n",
    "    placeholder=None, \n",
    "    # placeholder: DeltaGenerator | None=None,\n",
    ") -> None:\n",
    "    history.append(conversation)\n",
    "    # conversation.show(placeholder)\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    system: str | None,\n",
    "    tools: list[dict] | None,\n",
    "    history: list[Conversation],\n",
    ") -> str:\n",
    "    if tools:\n",
    "        tools = json.dumps(tools, indent=4, ensure_ascii=False)\n",
    "\n",
    "    prompt = f\"{Role.SYSTEM}\\n\"\n",
    "    prompt += system if not tools else TOOL_PROMPT\n",
    "    if tools:\n",
    "        tools = json.loads(tools)\n",
    "        prompt += json.dumps(tools, ensure_ascii=False)\n",
    "    for conversation in history:\n",
    "        prompt += f'{conversation}'\n",
    "    prompt += f'{Role.ASSISTANT}\\n'\n",
    "    return prompt\n",
    "\n",
    "def extract_code(text: str) -> str:\n",
    "    pattern = r'```([^\\n]*)\\n(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[-1][1]\n",
    "\n",
    "def tool_call(*args, **kwargs) -> dict:\n",
    "    print(\"=== Tool call:\")\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "tools = get_tools()\n",
    "print(\"===TOOLS \", tools)\n",
    "\n",
    "markdown_placeholder = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好\n",
      "assistant:  \n",
      " 你好，请问有什么我可以帮助你的吗？\n",
      "user: \n",
      "“大庇天下寒士俱欢颜”这句话有几个字\n",
      "assistant:  \n",
      " 这句话有8个字。\n",
      "user: \n",
      "请介绍一下华为最新款的手机\n",
      "assistant:  \n",
      " 这个问题需要我调用 web_search 函数来获取相关信息。\n",
      "assistant:   web_search\n",
      " ```python\n",
      "tool_call(keyword='华为最新款手机')\n",
      "```\n",
      "=== Tool call:\n",
      "()\n",
      "{'keyword': '华为最新款手机'}\n",
      "assistant:  \n",
      " 华为最新款的手机包括：HUAWEI Mate 60 Pro、HUAWEI Mate 60 Pro+、HUAWEI Mate X5、HUAWEI Mate 60 RS 和 P 系列。这些手机均搭载了先进的芯片和摄像头技术，支持5G网络和卫星通信。其中，HUAWEI Mate 60 Pro+和HUAWEI Mate 60 RS还支持双向北斗卫星消息，使您在户外环境中也能保持联系。P 系列手机则配备了超可靠昆仑玻璃和超聚光夜视长焦，适合拍摄高质量的照片。\n",
      "user: \n",
      "23的12次方是多少\n",
      "assistant:  \n",
      " 这个问题需要使用数学计算工具。\n",
      "assistant:   exponentiation_calculation\n",
      " ```python\n",
      "tool_call(base=23, power=12)\n",
      "```\n",
      "=== Tool call:\n",
      "()\n",
      "{'base': 23, 'power': 12}\n",
      "assistant:  \n",
      " 23的12次方等于21914624432020321。\n",
      "user: \n",
      "介绍一下苹果的最新手机\n",
      "assistant:  \n",
      " 这个问题需要我调用 web_search 函数来获取相关信息。\n",
      "assistant:   \n",
      " ```python\n",
      "tool_call(keyword='苹果最新手机')\n",
      "```\n",
      "assistant:  \n",
      " 苹果最新款的手机是iPhone 14，它拥有6.1英寸的屏幕，搭载了A16芯片，支持5G网络和iOS 15操作系统。iPhone 14还配备了双摄像头和超广角镜头，支持夜间拍摄和深度感知。此外，iPhone 14还具有 Face ID 和 Touch ID 功能，可以实现快速识别和支付。\n",
      "user: \n",
      "“大庇天下寒士俱欢颜”这句话有几个字\n",
      "assistant:  \n",
      " 这句话有8个字。\n",
      "user: \n",
      "不对吧\n",
      "assistant:  \n",
      " 很抱歉，我可能误解了您的问题。请问您能否提供更多的信息，让我更好地回答您的问题呢？\n",
      "user: \n",
      "END\n",
      "<|user|> 你好 None\n",
      "<|assistant|> 你好，请问有什么我可以帮助你的吗？ None\n",
      "<|user|> “大庇天下寒士俱欢颜”这句话有几个字 None\n",
      "<|assistant|> 这句话有8个字。 None\n",
      "<|user|> 请介绍一下华为最新款的手机 None\n",
      "<|assistant|> 这个问题需要我调用 web_search 函数来获取相关信息。 None\n",
      "<|assistant|> ```python\n",
      "tool_call(keyword='华为最新款手机')\n",
      "``` web_search\n",
      "<|observation|> HUAWEI Mate 60 Pro 了解更多 购买 最新 HUAWEI Mate 60 Pro+ 双卫星通信 超可靠玄武架构 了解更多 购买 最新 HUAWEI Mate X5 超轻薄全能折叠 玄武钢化昆仑玻璃 ￥12999 起 了解更多 购买 最新 HUAWEI Mate 60 RS 非凡大师 双卫星通信 玄武钢化昆仑玻璃 了解更多 购买 最新 HUAWEI Mate 60 双向北斗卫星消息 超可靠玄武架构 了解更多 购买 P 系列 超可靠昆仑玻璃 超聚光夜视长焦 双向北斗卫星消息，超强灵犀通信 None\n",
      "<|assistant|> 华为最新款的手机包括：HUAWEI Mate 60 Pro、HUAWEI Mate 60 Pro+、HUAWEI Mate X5、HUAWEI Mate 60 RS 和 P 系列。这些手机均搭载了先进的芯片和摄像头技术，支持5G网络和卫星通信。其中，HUAWEI Mate 60 Pro+和HUAWEI Mate 60 RS还支持双向北斗卫星消息，使您在户外环境中也能保持联系。P 系列手机则配备了超可靠昆仑玻璃和超聚光夜视长焦，适合拍摄高质量的照片。 None\n",
      "<|user|> 23的12次方是多少 None\n",
      "<|assistant|> 这个问题需要使用数学计算工具。 None\n",
      "<|assistant|> ```python\n",
      "tool_call(base=23, power=12)\n",
      "``` exponentiation_calculation\n",
      "<|observation|> 以23的12次方的计算结果为21914624432020321 None\n",
      "<|assistant|> 23的12次方等于21914624432020321。 None\n",
      "<|user|> 介绍一下苹果的最新手机 None\n",
      "<|assistant|> 这个问题需要我调用 web_search 函数来获取相关信息。 None\n",
      "<|assistant|> tool_call(keyword='苹果最新手机')\n",
      "``` ```python\n",
      "<|assistant|> 苹果最新款的手机是iPhone 14，它拥有6.1英寸的屏幕，搭载了A16芯片，支持5G网络和iOS 15操作系统。iPhone 14还配备了双摄像头和超广角镜头，支持夜间拍摄和深度感知。此外，iPhone 14还具有 Face ID 和 Touch ID 功能，可以实现快速识别和支付。 None\n",
      "<|user|> “大庇天下寒士俱欢颜”这句话有几个字 None\n",
      "<|assistant|> 这句话有8个字。 None\n",
      "<|user|> 不对吧 None\n",
      "<|assistant|> 很抱歉，我可能误解了您的问题。请问您能否提供更多的信息，让我更好地回答您的问题呢？ None\n"
     ]
    }
   ],
   "source": [
    "# Experiment\n",
    "import time\n",
    "history: list[Conversation] = []\n",
    "\n",
    "for i in range(10):\n",
    "# def dialogue(txt):\n",
    "    print('user: ', flush=True)\n",
    "    input_text = input()\n",
    "    print(input_text)\n",
    "    if input_text == \"END\":\n",
    "        break\n",
    "    input_text = input_text.strip()\n",
    "    role = Role.USER\n",
    "    append_conversation(Conversation(role, input_text), history)\n",
    "    # input_text = preprocess_text(\n",
    "    #     None,\n",
    "    #     tools,\n",
    "    #     history,\n",
    "    # )\n",
    "    \n",
    "    for _ in range(5):\n",
    "        output_text = ''\n",
    "        user_mark = 0\n",
    "        for response in client.generate_stream(\n",
    "            system=None,\n",
    "            tools=tools,\n",
    "            history=history,\n",
    "            do_sample=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            stop_sequences=[str(r) for r in (Role.USER, Role.OBSERVATION)],\n",
    "        ):\n",
    "            token = response.token\n",
    "            if response.token.special:\n",
    "                # print(output_text)\n",
    "                logging.info(output_text)\n",
    "                logging.info(token)\n",
    "                print('assistant: ', output_text)\n",
    "\n",
    "\n",
    "                match token.text.strip():\n",
    "                    case '<|user|>':\n",
    "                        # print('assistant: ', output_text)\n",
    "                        # time.sleep(1)\n",
    "                        append_conversation(Conversation(\n",
    "                            Role.ASSISTANT,\n",
    "                            postprocess_text(output_text),\n",
    "                        ), history, markdown_placeholder)\n",
    "                        user_mark = 1\n",
    "                        break\n",
    "                    # Initiate tool call\n",
    "                    case '<|assistant|>':\n",
    "                        append_conversation(Conversation(\n",
    "                            Role.ASSISTANT,\n",
    "                            postprocess_text(output_text),\n",
    "                        ), history, markdown_placeholder)\n",
    "                        output_text = ''\n",
    "                        # message_placeholder = placeholder.chat_message(name=\"tool\", avatar=\"assistant\")\n",
    "                        # markdown_placeholder = message_placeholder.empty()\n",
    "                        continue\n",
    "                    case '<|observation|>':\n",
    "                        tool, *output_text = output_text.strip().split('\\n')\n",
    "                        output_text = '\\n'.join(output_text)\n",
    "                        \n",
    "                        append_conversation(Conversation(\n",
    "                            Role.TOOL,\n",
    "                            postprocess_text(output_text),\n",
    "                            tool,\n",
    "                        ), history, markdown_placeholder)\n",
    "                        # message_placeholder = placeholder.chat_message(name=\"observation\", avatar=\"user\")\n",
    "                        # markdown_placeholder = message_placeholder.empty()\n",
    "                        \n",
    "                        try:\n",
    "                            code = extract_code(output_text)\n",
    "                            logging.info(f\"CODE: {code}\")\n",
    "                            \n",
    "                            args = eval(code, {'tool_call': tool_call}, {})\n",
    "                        except:\n",
    "                            logging.warning('Failed to parse tool call')\n",
    "                            break\n",
    "                        \n",
    "                        output_text = ''\n",
    "                        \n",
    "                        # if manual_mode:\n",
    "                        #     st.info('Please provide tool call results below:')\n",
    "                        #     return\n",
    "                        # else:\n",
    "                        #     with markdown_placeholder:\n",
    "                        #         with st.spinner(f'Calling tool {tool}...'):\n",
    "                        #             observation = dispatch_tool(tool, args)\n",
    "                        observation = dispatch_tool(tool, args)\n",
    "                        \n",
    "                        if len(observation) > TRUNCATE_LENGTH:\n",
    "                            observation = observation[:TRUNCATE_LENGTH] + ' [TRUNCATED]'\n",
    "                        append_conversation(Conversation(\n",
    "                            Role.OBSERVATION, observation\n",
    "                        ), history, markdown_placeholder)\n",
    "                        # message_placeholder = placeholder.chat_message(name=\"assistant\", avatar=\"assistant\")\n",
    "                        # markdown_placeholder = message_placeholder.empty()\n",
    "                        # st.session_state.calling_tool = False\n",
    "                        break\n",
    "                    case _:\n",
    "                        logging.warning(f'Unexpected special token: {token.text.strip()}')\n",
    "                        break\n",
    "            output_text += response.token.text\n",
    "            # markdown_placeholder.markdown(postprocess_text(output_text + '▌'))\n",
    "        else:\n",
    "            append_conversation(Conversation(\n",
    "                Role.ASSISTANT,\n",
    "                postprocess_text(output_text),\n",
    "            ), history, markdown_placeholder)\n",
    "            user_mark = 1\n",
    "            print('assistant: ', output_text)\n",
    "        if user_mark:\n",
    "            break\n",
    "\n",
    "for h in history:\n",
    "    logging.info(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> 你好 None\n",
      "<|assistant|> 你好，请问有什么我可以帮助你的吗？ None\n",
      "<|user|> 请介绍一下华为的最新款手机 None\n",
      "<|assistant|> 这个问题需要我调用搜索引擎来获取相关信息。 None\n",
      "<|assistant|> ```python\n",
      "tool_call(keyword='华为最新款手机')\n",
      "``` web_search\n",
      "<|observation|> HUAWEI Mate 60 Pro 了解更多 购买 最新 HUAWEI Mate 60 Pro+ 双卫星通信 超可靠玄武架构 了解更多 购买 最新 HUAWEI Mate X5 超轻薄全能折叠 玄武钢化昆仑玻璃 ￥12999 起 了解更多 购买 最新 HUAWEI Mate 60 RS 非凡大师 双卫星通信 玄武钢化昆仑玻璃 了解更多 购买 最新 HUAWEI Mate 60 双向北斗卫星消息 超可靠玄武架构 了解更多 购买 P 系列 超可靠昆仑玻璃 超聚光夜视长焦 双向北斗卫星消息，超强灵犀通信 None\n",
      "<|assistant|> 华为最新款手机包括：HUAWEI Mate 60 Pro、HUAWEI Mate 60 Pro+、HUAWEI Mate X5、HUAWEI Mate 60 RS、HUAWEI Mate 60。这些手机均搭载了超可靠的昆仑玻璃，并且支持双向北斗卫星消息。其中，HUAWEI Mate 60 Pro+和HUAWEI Mate 60 RS还支持双卫星通信。此外，HUAWEI Mate X5是一款超轻薄的全能折叠手机，HUAWEI Mate 60则是一款售价更亲民的手机，具有超聚光夜视长焦功能。 None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Welcome to Python.org', 'href': 'https://www.python.org/', 'body': 'The core of extensible programming is defining functions. Python allows mandatory and optional arguments, keyword arguments, and even arbitrary argument lists. More about defining functions in Python 3. Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More.'}, {'title': 'Python Tutorial - W3Schools', 'href': 'https://www.w3schools.com/python/default.asp', 'body': 'Python is a popular programming language. Python can be used on a server to create web applications. Start learning Python now ».'}, {'title': 'Python (programming language) - Wikipedia', 'href': 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'body': 'Python is a high-level, general-purpose programming language.Its design philosophy emphasizes code readability with the use of significant indentation.. Python is dynamically typed and garbage-collected.It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.It is often described as a \"batteries included\" language ...'}, {'title': 'Python For Beginners | Python.org', 'href': 'https://www.python.org/about/gettingstarted/', 'body': 'Learning. Before getting started, you may want to find out which IDEs and text editors are tailored to make Python editing easy, browse the list of introductory books, or look at code samples that you might find helpful.. There is a list of tutorials suitable for experienced programmers on the BeginnersGuide/Tutorials page. There is also a list of resources in other languages which might be ...'}, {'title': 'The Python Tutorial — Python 3.12.0 documentation', 'href': 'https://docs.python.org/3/tutorial/index.html', 'body': \"Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python's elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on ...\"}]\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "with DDGS() as ddgs:\n",
    "    results = [r for r in ddgs.text(\"python programming\", max_results=5)]\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm3_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
