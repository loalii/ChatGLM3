{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "import logging\n",
    "import datetime\n",
    "log_time = datetime.datetime.now()\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "DATE_FORMAT = \"%m/%d/%Y %H:%M:%S %p\"\n",
    "logging.basicConfig(filename=f\"./logger/{log_time}.log\", level=logging.INFO, format=LOG_FORMAT,datefmt=DATE_FORMAT)\n",
    "logging.info(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[registered tool] {'description': '随机生成一个数x, 使得 `range[0]` <= x < `range[1]`， 随机数生成的种子使用 `seed`',\n",
      " 'name': 'random_number_generator',\n",
      " 'params': [{'description': '随机数生成器使用的种子',\n",
      "             'name': 'seed',\n",
      "             'required': True,\n",
      "             'type': 'int'},\n",
      "            {'description': '生成随机数的范围',\n",
      "             'name': 'range',\n",
      "             'required': True,\n",
      "             'type': 'tuple[int, int]'}]}\n",
      "[registered tool] {'description': '获取句子 `input_text` 的长度',\n",
      " 'name': 'get_sentence_length',\n",
      " 'params': [{'description': '输入的句子',\n",
      "             'name': 'input_text',\n",
      "             'required': True,\n",
      "             'type': 'str'}]}\n",
      "[registered tool] {'description': '返回指数计算的结果，底数 `base` 的指数 `power` 次方',\n",
      " 'name': 'exponentiation_calculation',\n",
      " 'params': [{'description': '底数',\n",
      "             'name': 'base',\n",
      "             'required': True,\n",
      "             'type': 'int'},\n",
      "            {'description': '指数',\n",
      "             'name': 'power',\n",
      "             'required': True,\n",
      "             'type': 'int'}]}\n",
      "[registered tool] {'description': '从网络上获得 `keyword` 的习惯内容信息。\\n'\n",
      "                '在你要回答你现有知识无法回答的问题时，你应该使用这个工具tool（尤其是当你需要获得最新的实时信息，或者你缺少相关信息时，在这种情况下请更倾向于使用他）。',\n",
      " 'name': 'web_search',\n",
      " 'params': [{'description': '搜索使用的关键字',\n",
      "             'name': 'keyword',\n",
      "             'required': True,\n",
      "             'type': 'str'}]}\n",
      "[registered tool] {'description': 'Get the current weather for `city_name`',\n",
      " 'name': 'get_weather',\n",
      " 'params': [{'description': 'The name of the city to be queried',\n",
      "             'name': 'city_name',\n",
      "             'required': True,\n",
      "             'type': 'str'}]}\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "import os\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "from huggingface_hub.inference._text_generation import TextGenerationStreamResponse, Token\n",
    "from collections.abc import Iterable\n",
    "from typing import Any, Protocol\n",
    "\n",
    "import json\n",
    "import re\n",
    "from conversation import postprocess_text, preprocess_text, Conversation, Role\n",
    "from tool_registry import dispatch_tool, get_tools\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"]='http://10.10.20.100:1089'\n",
    "os.environ[\"HTTPS_PROXY\"]='http://10.10.20.100:1089'\n",
    "\n",
    "TOOL_PROMPT = 'Answer the following questions as best as you can. You have access to the following tools:'\n",
    "\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH', '/share/lilin/chatglm3-6b')\n",
    "# MODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/chatglm3-6b')\n",
    "PT_PATH = os.environ.get('PT_PATH', None)       # 不使用checkpoint\n",
    "TOKENIZER_PATH = os.environ.get(\"TOKENIZER_PATH\", MODEL_PATH)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/share/lilin/chatglm3-6b\", trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(\"/share/lilin/chatglm3-6b\", trust_remote_code=True, device='cuda')\n",
    "# model = model.eval()\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "# print(response)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b488286add24660b1e24aec094bbd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 挂起llm\n",
    "def stream_chat(model, tokenizer, query: str, history: list[tuple[str, str]] = None, role: str = \"user\",\n",
    "                    past_key_values=None,max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8,\n",
    "                    logits_processor=None, return_past_key_values=False, **kwargs):\n",
    "        \n",
    "    from transformers.generation.logits_process import LogitsProcessor\n",
    "    from transformers.generation.utils import LogitsProcessorList\n",
    "\n",
    "    class InvalidScoreLogitsProcessor(LogitsProcessor):\n",
    "        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "            if torch.isnan(scores).any() or torch.isinf(scores).any():\n",
    "                scores.zero_()\n",
    "                scores[..., 5] = 5e4\n",
    "            return scores\n",
    "\n",
    "    if history is None:\n",
    "        history = []\n",
    "    if logits_processor is None:\n",
    "        logits_processor = LogitsProcessorList()\n",
    "    logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "    eos_token_id = [tokenizer.eos_token_id, tokenizer.get_command(\"<|user|>\"),\n",
    "                    tokenizer.get_command(\"<|observation|>\")]\n",
    "    gen_kwargs = {\"max_length\": max_length, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "                    \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n",
    "    if past_key_values is None:\n",
    "        inputs = tokenizer.build_chat_input(query, history=history, role=role)\n",
    "    else:\n",
    "        inputs = tokenizer.build_chat_input(query, role=role)\n",
    "    inputs = inputs.to(model.device)\n",
    "    if past_key_values is not None:\n",
    "        past_length = past_key_values[0][0].shape[0]\n",
    "        if model.transformer.pre_seq_len is not None:\n",
    "            past_length -= model.transformer.pre_seq_len\n",
    "        inputs.position_ids += past_length\n",
    "        attention_mask = inputs.attention_mask\n",
    "        attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    history.append({\"role\": role, \"content\": query})\n",
    "    for outputs in model.stream_generate(**inputs, past_key_values=past_key_values,\n",
    "                                        eos_token_id=eos_token_id, return_past_key_values=return_past_key_values,\n",
    "                                        **gen_kwargs):\n",
    "        if return_past_key_values:\n",
    "            outputs, past_key_values = outputs\n",
    "        outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):]\n",
    "        response = tokenizer.decode(outputs)\n",
    "        if response and response[-1] != \"�\":\n",
    "            new_history = history\n",
    "            if return_past_key_values:\n",
    "                yield response, new_history, past_key_values\n",
    "            else:\n",
    "                yield response, new_history\n",
    "\n",
    "# class Client(Protocol):         # 协议类只用于代码静态检查\n",
    "#     def generate_stream(self,\n",
    "#         system: str | None,\n",
    "#         tools: list[dict] | None,\n",
    "#         history: list[Conversation],\n",
    "#         **parameters: Any\n",
    "#     ) -> Iterable[TextGenerationStreamResponse]:\n",
    "#         ...\n",
    "\n",
    "OBS_PROMPT = \"You have used tools and got the related information. Using the following tool results answering the previous questions: \"\n",
    "\n",
    "\n",
    "class HFClient:\n",
    "    def __init__(self, model_path: str, tokenizer_path: str, pt_checkpoint: str | None = None,):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "\n",
    "        if pt_checkpoint is not None:\n",
    "            config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)\n",
    "            self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True, config=config)\n",
    "            prefix_state_dict = torch.load(os.path.join(pt_checkpoint, \"pytorch_model.bin\"))\n",
    "            new_prefix_state_dict = {}\n",
    "            for k, v in prefix_state_dict.items():\n",
    "                if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "                    new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "            print(\"Loaded from pt checkpoints\", new_prefix_state_dict.keys())\n",
    "            self.model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        self.model = self.model.to(\n",
    "            'cuda' if torch.cuda.is_available() else\n",
    "            'mps' if torch.backends.mps.is_available() else\n",
    "            'cpu'\n",
    "        ).eval()\n",
    "\n",
    "\n",
    "    def generate_stream(self,\n",
    "        system: str | None,\n",
    "        tools: list[dict] | None,\n",
    "        history: list[Conversation],\n",
    "        **parameters: Any\n",
    "    ) -> Iterable[TextGenerationStreamResponse]:\n",
    "\n",
    "        chat_history = []\n",
    "\n",
    "        for conversation in history[:-1]:\n",
    "            chat_history.append({\n",
    "                'role': str(conversation.role).removeprefix('<|').removesuffix('|>'),\n",
    "                'content': conversation.content,\n",
    "            })\n",
    "        \n",
    "        chat_history.append({\n",
    "            'role': 'system',\n",
    "            'content': OBS_PROMPT if history[-1].role==Role.OBSERVATION else TOOL_PROMPT,\n",
    "        })\n",
    "\n",
    "        if tools:\n",
    "            chat_history[-1]['tools'] = tools\n",
    "\n",
    "        # chat_history.append({\n",
    "        #         'role': str(history[-1].role).removeprefix('<|').removesuffix('|>'),\n",
    "        #         'content': history[-1].content,\n",
    "        #     })\n",
    "\n",
    "\n",
    "        query = history[-1].content\n",
    "        role = str(history[-1].role).removeprefix('<|').removesuffix('|>')\n",
    "\n",
    "        text = ''\n",
    "        \n",
    "        for new_text, _ in stream_chat(self.model,\n",
    "            self.tokenizer,\n",
    "            query,\n",
    "            chat_history,\n",
    "            role,\n",
    "            **parameters,\n",
    "        ):\n",
    "            word = new_text.removeprefix(text)\n",
    "            word_stripped = word.strip()\n",
    "            text = new_text\n",
    "            yield TextGenerationStreamResponse(\n",
    "                generated_text=text,\n",
    "                token=Token(\n",
    "                    id=0,\n",
    "                    logprob=0,\n",
    "                    text=word,\n",
    "                    special=word_stripped.startswith('<|') and word_stripped.endswith('|>'),\n",
    "                )\n",
    "            )\n",
    "\n",
    "client = HFClient(MODEL_PATH, TOKENIZER_PATH, PT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===TOOLS  {'random_number_generator': {'name': 'random_number_generator', 'description': '随机生成一个数x, 使得 `range[0]` <= x < `range[1]`， 随机数生成的种子使用 `seed`', 'params': [{'name': 'seed', 'description': '随机数生成器使用的种子', 'type': 'int', 'required': True}, {'name': 'range', 'description': '生成随机数的范围', 'type': 'tuple[int, int]', 'required': True}]}, 'get_sentence_length': {'name': 'get_sentence_length', 'description': '获取句子 `input_text` 的长度', 'params': [{'name': 'input_text', 'description': '输入的句子', 'type': 'str', 'required': True}]}, 'exponentiation_calculation': {'name': 'exponentiation_calculation', 'description': '返回指数计算的结果，底数 `base` 的指数 `power` 次方', 'params': [{'name': 'base', 'description': '底数', 'type': 'int', 'required': True}, {'name': 'power', 'description': '指数', 'type': 'int', 'required': True}]}, 'web_search': {'name': 'web_search', 'description': '从网络上获得 `keyword` 的习惯内容信息。\\n在你要回答你现有知识无法回答的问题时，你应该使用这个工具tool（尤其是当你需要获得最新的实时信息，或者你缺少相关信息时，在这种情况下请更倾向于使用他）。', 'params': [{'name': 'keyword', 'description': '搜索使用的关键字', 'type': 'str', 'required': True}]}, 'get_weather': {'name': 'get_weather', 'description': 'Get the current weather for `city_name`', 'params': [{'name': 'city_name', 'description': 'The name of the city to be queried', 'type': 'str', 'required': True}]}}\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "TOP_P = 0.8\n",
    "TEMPERATURE = 0.01\n",
    "MAX_LENGTH = 8192\n",
    "TRUNCATE_LENGTH = 1024\n",
    "\n",
    "\n",
    "def append_conversation(\n",
    "    conversation: Conversation,\n",
    "    history: list[Conversation],\n",
    "    placeholder=None, \n",
    "    # placeholder: DeltaGenerator | None=None,\n",
    ") -> None:\n",
    "    history.append(conversation)\n",
    "    # conversation.show(placeholder)\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    system: str | None,\n",
    "    tools: list[dict] | None,\n",
    "    history: list[Conversation],\n",
    ") -> str:\n",
    "    if tools:\n",
    "        tools = json.dumps(tools, indent=4, ensure_ascii=False)\n",
    "\n",
    "    prompt = f\"{Role.SYSTEM}\\n\"\n",
    "    prompt += system if not tools else TOOL_PROMPT\n",
    "    if tools:\n",
    "        tools = json.loads(tools)\n",
    "        prompt += json.dumps(tools, ensure_ascii=False)\n",
    "    for conversation in history:\n",
    "        prompt += f'{conversation}'\n",
    "    prompt += f'{Role.ASSISTANT}\\n'\n",
    "    return prompt\n",
    "\n",
    "def extract_code(text: str) -> str:\n",
    "    pattern = r'```([^\\n]*)\\n(.*?)```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[-1][1]\n",
    "\n",
    "def tool_call(*args, **kwargs) -> dict:\n",
    "    print(\"=== Tool call:\")\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "tools = get_tools()\n",
    "print(\"===TOOLS \", tools)\n",
    "\n",
    "markdown_placeholder = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "”大庇天下寒士俱欢颜“这句话有几个字\n",
      "assistant:  get_sentence_length\n",
      " ```python\n",
      "tool_call(input_text='大庇天下寒士俱欢颜')\n",
      "```\n",
      "=== Tool call:\n",
      "()\n",
      "{'input_text': '大庇天下寒士俱欢颜'}\n",
      "assistant:  \n",
      " 这句话“大庇天下寒士俱欢颜”的长度为9个字。\n",
      "user: \n",
      "请介绍一下华为最新款的手机\n",
      "assistant:  \n",
      " 这个问题需要我调用网络搜索工具来获取相关信息。\n",
      "assistant:   web_search\n",
      " ```python\n",
      "tool_call(keyword='华为最新款手机')\n",
      "```\n",
      "=== Tool call:\n",
      "()\n",
      "{'keyword': '华为最新款手机'}\n",
      "assistant:  \n",
      " 华为最新款的手机包括：HUAWEI Mate X5、HUAWEI Mate 60 RS、HUAWEI Mate 60和P系列等。其中，HUAWEI Mate X5 是超轻薄全能折叠手机，采用玄武钢化昆仑玻璃；HUAWEI Mate 60 RS 是非凡大师双卫星通信手机，采用玄武钢化昆仑玻璃；HUAWEI Mate 60 是双向北斗卫星消息超可靠玄武架构手机，采用超可靠昆仑玻璃；P系列则采用了超可靠昆仑玻璃和双向北斗卫星消息等配置。\n",
      "user: \n",
      "再介绍一下苹果最新款的手机\n",
      "assistant:  \n",
      " 由于我无法获取到苹果最新款手机的具体信息，但根据往年的经验，苹果每年都会在秋季发布新款手机，这些手机通常会搭载最新的苹果芯片，拥有更快的处理器和更先进的功能。同时，苹果手机的设计和用户体验也一直备受好评，因此，苹果手机一直备受全球用户的喜爱。\n",
      "user: \n",
      "你可以到网上进行搜索的\n",
      "assistant:  \n",
      " 当然可以，我可以使用网络搜索工具获取相关信息。请问您需要我帮您搜索什么内容？\n",
      "user: \n",
      "苹果的最新款手机\n",
      "assistant:  \n",
      " 很抱歉，由于我无法获取到苹果最新款手机的具体信息，但根据往年的经验，苹果每年都会在秋季发布新款手机，这些手机通常会搭载最新的苹果芯片，拥有更快的处理器和更先进的功能。同时，苹果手机的设计和用户体验也一直备受好评，因此，苹果手机一直备受全球用户的喜爱。\n",
      "user: \n",
      "END\n",
      "<|user|> ”大庇天下寒士俱欢颜“这句话有几个字 None\n",
      "<|assistant|> ```python\n",
      "tool_call(input_text='大庇天下寒士俱欢颜')\n",
      "``` get_sentence_length\n",
      "<|observation|> 这句话大庇天下寒士俱欢颜的长度为9 None\n",
      "<|assistant|> 这句话“大庇天下寒士俱欢颜”的长度为9个字。 None\n",
      "<|user|> 请介绍一下华为最新款的手机 None\n",
      "<|assistant|> 这个问题需要我调用网络搜索工具来获取相关信息。 None\n",
      "<|assistant|> ```python\n",
      "tool_call(keyword='华为最新款手机')\n",
      "``` web_search\n",
      "<|observation|> HUAWEI Mate X5 超轻薄全能折叠 玄武钢化昆仑玻璃 ￥12999 起 了解更多 购买 最新 HUAWEI Mate 60 RS 非凡大师 双卫星通信 玄武钢化昆仑玻璃 了解更多 购买 最新 HUAWEI Mate 60 双向北斗卫星消息 超可靠玄武架构 了解更多 购买 P 系列 超可靠昆仑玻璃 超聚光夜视长焦 双向北斗卫星消息，超强灵犀通信 None\n",
      "<|assistant|> 华为最新款的手机包括：HUAWEI Mate X5、HUAWEI Mate 60 RS、HUAWEI Mate 60和P系列等。其中，HUAWEI Mate X5 是超轻薄全能折叠手机，采用玄武钢化昆仑玻璃；HUAWEI Mate 60 RS 是非凡大师双卫星通信手机，采用玄武钢化昆仑玻璃；HUAWEI Mate 60 是双向北斗卫星消息超可靠玄武架构手机，采用超可靠昆仑玻璃；P系列则采用了超可靠昆仑玻璃和双向北斗卫星消息等配置。 None\n",
      "<|user|> 再介绍一下苹果最新款的手机 None\n",
      "<|assistant|> 由于我无法获取到苹果最新款手机的具体信息，但根据往年的经验，苹果每年都会在秋季发布新款手机，这些手机通常会搭载最新的苹果芯片，拥有更快的处理器和更先进的功能。同时，苹果手机的设计和用户体验也一直备受好评，因此，苹果手机一直备受全球用户的喜爱。 None\n",
      "<|user|> 你可以到网上进行搜索的 None\n",
      "<|assistant|> 当然可以，我可以使用网络搜索工具获取相关信息。请问您需要我帮您搜索什么内容？ None\n",
      "<|user|> 苹果的最新款手机 None\n",
      "<|assistant|> 很抱歉，由于我无法获取到苹果最新款手机的具体信息，但根据往年的经验，苹果每年都会在秋季发布新款手机，这些手机通常会搭载最新的苹果芯片，拥有更快的处理器和更先进的功能。同时，苹果手机的设计和用户体验也一直备受好评，因此，苹果手机一直备受全球用户的喜爱。 None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "history: list[Conversation] = []\n",
    "\n",
    "for i in range(10):\n",
    "# def dialogue(txt):\n",
    "    print('user: ', flush=True)\n",
    "    input_text = input()\n",
    "    print(input_text)\n",
    "    if input_text == \"END\":\n",
    "        break\n",
    "    input_text = input_text.strip()\n",
    "    role = Role.USER\n",
    "    append_conversation(Conversation(role, input_text), history)\n",
    "    # input_text = preprocess_text(\n",
    "    #     None,\n",
    "    #     tools,\n",
    "    #     history,\n",
    "    # )\n",
    "    \n",
    "    for _ in range(5):\n",
    "        output_text = ''\n",
    "        user_mark = 0\n",
    "        for response in client.generate_stream(\n",
    "            system=None,\n",
    "            tools=tools,\n",
    "            history=history,\n",
    "            do_sample=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            stop_sequences=[str(r) for r in (Role.USER, Role.OBSERVATION)],\n",
    "        ):\n",
    "            token = response.token\n",
    "            if response.token.special:\n",
    "                # print(output_text)\n",
    "                logging.info(output_text)\n",
    "                logging.info(token)\n",
    "                print('assistant: ', output_text)\n",
    "\n",
    "\n",
    "                match token.text.strip():\n",
    "                    case '<|user|>':\n",
    "                        # print('assistant: ', output_text)\n",
    "                        # time.sleep(1)\n",
    "                        append_conversation(Conversation(\n",
    "                            Role.ASSISTANT,\n",
    "                            postprocess_text(output_text),\n",
    "                        ), history, markdown_placeholder)\n",
    "                        user_mark = 1\n",
    "                        break\n",
    "                    # Initiate tool call\n",
    "                    case '<|assistant|>':\n",
    "                        append_conversation(Conversation(\n",
    "                            Role.ASSISTANT,\n",
    "                            postprocess_text(output_text),\n",
    "                        ), history, markdown_placeholder)\n",
    "                        output_text = ''\n",
    "                        # message_placeholder = placeholder.chat_message(name=\"tool\", avatar=\"assistant\")\n",
    "                        # markdown_placeholder = message_placeholder.empty()\n",
    "                        continue\n",
    "                    case '<|observation|>':\n",
    "                        tool, *output_text = output_text.strip().split('\\n')\n",
    "                        output_text = '\\n'.join(output_text)\n",
    "                        \n",
    "                        append_conversation(Conversation(\n",
    "                            Role.TOOL,\n",
    "                            postprocess_text(output_text),\n",
    "                            tool,\n",
    "                        ), history, markdown_placeholder)\n",
    "                        # message_placeholder = placeholder.chat_message(name=\"observation\", avatar=\"user\")\n",
    "                        # markdown_placeholder = message_placeholder.empty()\n",
    "                        \n",
    "                        try:\n",
    "                            code = extract_code(output_text)\n",
    "                            logging.info(f\"CODE: {code}\")\n",
    "                            \n",
    "                            args = eval(code, {'tool_call': tool_call}, {})\n",
    "                        except:\n",
    "                            logging.warning('Failed to parse tool call')\n",
    "                            break\n",
    "                        \n",
    "                        output_text = ''\n",
    "                        \n",
    "                        # if manual_mode:\n",
    "                        #     st.info('Please provide tool call results below:')\n",
    "                        #     return\n",
    "                        # else:\n",
    "                        #     with markdown_placeholder:\n",
    "                        #         with st.spinner(f'Calling tool {tool}...'):\n",
    "                        #             observation = dispatch_tool(tool, args)\n",
    "                        observation = dispatch_tool(tool, args)\n",
    "                        \n",
    "                        if len(observation) > TRUNCATE_LENGTH:\n",
    "                            observation = observation[:TRUNCATE_LENGTH] + ' [TRUNCATED]'\n",
    "                        append_conversation(Conversation(\n",
    "                            Role.OBSERVATION, observation\n",
    "                        ), history, markdown_placeholder)\n",
    "                        # message_placeholder = placeholder.chat_message(name=\"assistant\", avatar=\"assistant\")\n",
    "                        # markdown_placeholder = message_placeholder.empty()\n",
    "                        # st.session_state.calling_tool = False\n",
    "                        break\n",
    "                    case _:\n",
    "                        logging.warning(f'Unexpected special token: {token.text.strip()}')\n",
    "                        break\n",
    "            output_text += response.token.text\n",
    "            # markdown_placeholder.markdown(postprocess_text(output_text + '▌'))\n",
    "        else:\n",
    "            append_conversation(Conversation(\n",
    "                Role.ASSISTANT,\n",
    "                postprocess_text(output_text),\n",
    "            ), history, markdown_placeholder)\n",
    "            user_mark = 1\n",
    "            print('assistant: ', output_text)\n",
    "        if user_mark:\n",
    "            break\n",
    "\n",
    "for h in history:\n",
    "    logging.info(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> 你好 None\n",
      "<|assistant|> 你好，请问有什么我可以帮助你的吗？ None\n",
      "<|user|> 请介绍一下华为的最新款手机 None\n",
      "<|assistant|> 这个问题需要我调用搜索引擎来获取相关信息。 None\n",
      "<|assistant|> ```python\n",
      "tool_call(keyword='华为最新款手机')\n",
      "``` web_search\n",
      "<|observation|> HUAWEI Mate 60 Pro 了解更多 购买 最新 HUAWEI Mate 60 Pro+ 双卫星通信 超可靠玄武架构 了解更多 购买 最新 HUAWEI Mate X5 超轻薄全能折叠 玄武钢化昆仑玻璃 ￥12999 起 了解更多 购买 最新 HUAWEI Mate 60 RS 非凡大师 双卫星通信 玄武钢化昆仑玻璃 了解更多 购买 最新 HUAWEI Mate 60 双向北斗卫星消息 超可靠玄武架构 了解更多 购买 P 系列 超可靠昆仑玻璃 超聚光夜视长焦 双向北斗卫星消息，超强灵犀通信 None\n",
      "<|assistant|> 华为最新款手机包括：HUAWEI Mate 60 Pro、HUAWEI Mate 60 Pro+、HUAWEI Mate X5、HUAWEI Mate 60 RS、HUAWEI Mate 60。这些手机均搭载了超可靠的昆仑玻璃，并且支持双向北斗卫星消息。其中，HUAWEI Mate 60 Pro+和HUAWEI Mate 60 RS还支持双卫星通信。此外，HUAWEI Mate X5是一款超轻薄的全能折叠手机，HUAWEI Mate 60则是一款售价更亲民的手机，具有超聚光夜视长焦功能。 None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Welcome to Python.org', 'href': 'https://www.python.org/', 'body': 'The core of extensible programming is defining functions. Python allows mandatory and optional arguments, keyword arguments, and even arbitrary argument lists. More about defining functions in Python 3. Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More.'}, {'title': 'Python Tutorial - W3Schools', 'href': 'https://www.w3schools.com/python/default.asp', 'body': 'Python is a popular programming language. Python can be used on a server to create web applications. Start learning Python now ».'}, {'title': 'Python (programming language) - Wikipedia', 'href': 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'body': 'Python is a high-level, general-purpose programming language.Its design philosophy emphasizes code readability with the use of significant indentation.. Python is dynamically typed and garbage-collected.It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.It is often described as a \"batteries included\" language ...'}, {'title': 'Python For Beginners | Python.org', 'href': 'https://www.python.org/about/gettingstarted/', 'body': 'Learning. Before getting started, you may want to find out which IDEs and text editors are tailored to make Python editing easy, browse the list of introductory books, or look at code samples that you might find helpful.. There is a list of tutorials suitable for experienced programmers on the BeginnersGuide/Tutorials page. There is also a list of resources in other languages which might be ...'}, {'title': 'The Python Tutorial — Python 3.12.0 documentation', 'href': 'https://docs.python.org/3/tutorial/index.html', 'body': \"Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python's elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on ...\"}]\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "with DDGS() as ddgs:\n",
    "    results = [r for r in ddgs.text(\"python programming\", max_results=5)]\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "157.58648490814926\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    a = float(input())\n",
    "    print(i**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm3_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
